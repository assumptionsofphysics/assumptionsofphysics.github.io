---
title: It’s not about mechanisms
summary: Explanations at the fundamental level cannot be about underlying mechanisms, they must be about required definitions
category: Foundations of Physics
tags: [Foundations of Physics, Reductionism, Fundamentality]
---

Working among physicists, I have noticed that some of them have an obsession, almost a fetish, with mechanistic explanations. Let me give you an example. Many years ago I was at an interdisciplinary workshop on the use of entropy. One of the talks was given by an evolutionary biologist that showed how the number of species (e.g. herbivores, carnivores, etc…) on an island could be predicted using entropy maximization, a technique borrowed from statistical mechanics. The final distribution depended only the size of the island and it gave a good experimental fit. I was blown away, which is expected as I am a sucker for arguments that take as input a small number of generic assumptions.

At lunch, I sat with a couple of physicists who were dismissing the result as insignificant. They argued that the result didn’t really say anything because it didn’t tell us the actual mechanism that causes those exact numbers of species. I explained that the result shows that the mechanism itself is irrelevant: any mechanism that increases entropy, that explores enough of the space of possibilities, will yield the same result. At first approximation, the only relevant thing about the mechanism is that it increases entropy. They stared at me for a few seconds, and then one added, “Yes… but what is the mechanism?” They genuinely couldn’t contemplate that the actual mechanism didn’t matter.

Most of the time, physicists do work with mechanisms and mechanistic explanations, so the bias is justified. Roughly speaking, a physical theory defines some objects together with some law that describes their behavior. For example, in fluid dynamics we have fluids and the Navier-Stokes equations. In classical electromagnetism, you have the fields with Maxwell’s equations. If one asks why we have those equations with those particular parameters, the typical answer is to go to a more fundamental theory. Therefore, if we ask why water has that particular coefficient for viscosity or surface tension, the answer is given by the bonds between molecules, which are described by chemistry and, ultimately, quantum mechanics.

This bias is ultimately a problem when working on the foundations: if a deeper physical explanation is given only by a lower-level theory, what do you do for a fundamental theory? The typical approach seems to be to find an even more fundamental theory. Why are measurements in quantum mechanics projections, and not unitary evolutions? You posit some more fundamental objects with some law that reproduced that behavior. Why is space-time described by a set of continuous quantities? You posit some more fundamental structure from which standard space-time emerges. But that just delays the problem: how are you going to explain those theories?

In a truly fundamental theory, then, the behavior of the objects cannot be explained by a more fundamental theory. The only thing that can explain it, then, is the very nature of the objects: they behave like that because, by definition, they must. This is what happens in mathematics: the integral is the anti-derivative because how they are defined. So, how can this work for physics?

Suppose we want to study physical process. This will take some input and return an output. Given that, in general, a process can be non-deterministic or non-reversible, it will take a statistical ensemble as input and returning another statistical ensemble as output. So a process is a map between statistical ensembles by definition. Now, ensembles allow mixtures: we can construct ensemble $$ C $$ by taking ensemble $$ A $$ 30% of the time and ensemble $$ B $$ 70% of the time. So we can write $$ C = p_A A + p_B B $$. Again, that’s the nature of statistical mixtures. But given that a process $$ P $$ will always act, at each time, on a particular instance of the mixture, applying a process to a mixture of inputes must be the same as mixing the outputs of the process applied to each input: $$ P(p_A A + p_B B) = p_A P(A) + p_B P(B) $$. Again, this is just because of what mixtures and processes are. Now, if we have a process $$ M $$ that we can use to perform a measurement, we would want it to be repeatable: if we apply the process twice we get the same result. That is $$ M(M(A))=M(A) $$. Mathematically, then, $$ M $$ is a linear operator that is idempotent, but that is exactly the definition of a projection. Therefore, measurements are modeled as projections, not because of some underlying mechanism that happens during a process, but because that’s what they must be. We select processes that act as projections because that’s the desired behavior for a measurement. The mechanism is important only insomuch that it guarantees the behavior.

A truly fundamental theory, then, is not found when we have found the true objects the universe is made of and how they work. It is found when we identify those definitions and requirements that are intrinsic to the objects we are set to describe. Those are the truly fundamental building blocks of physics.
