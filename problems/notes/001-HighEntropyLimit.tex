\documentclass[11pt]{article}

\usepackage[margin=1.25in]{geometry}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{enumitem}

\def\>{\rangle}
\def\<{\langle}
\DeclareMathOperator{\Tr}{Tr}

\newtheorem{prop}{Proposition}

\begin{document}


	\title{Notes on problem 1 - Classical physics as high entropy limit}
	\author{Assumptions of Physics Collaboration}

	\date{}

	\maketitle

\emph{NB: these are rough notes on our progress towards an open problem}

\section{Mathematical setting}

Let $H$ be the $L^2(\mathbb{R})$ Hilbert space. Let $M(H)$ be the space of mixed states, that is the space of all positive semi-definite Hermitian operators with trace one. Let $I(\rho) = \Tr [-\rho \log \rho]$ be the entropy of the mixed state Let $M(I_0) = I^{-1}(I_0)$ be the set of mixed state with the given entropy (i.e. fiber of $I_0$).

Let $\sigma_q \in \mathbb{R}$ be a fixed uncertainty (e.g. $\sigma_q = \sqrt{\hbar/2}$ expressed in meters, so that the uncertainty is balanced between $q$ and $p$). A Gaussian wave packet $|\gamma(\mu_q,\mu_p)\>$ is a pure state of the form
\begin{equation}
	|\gamma(\mu_q,\mu_p)\> = \frac{1}{(2 \pi \sigma_q^2)^{\frac{1}{4}}} e^{-\frac{1}{4} \left(\frac{q-\mu_q}{\sigma_q}\right)^2} e^{\imath \frac{\mu_p q}{\hbar}} | q \>
\end{equation}
or in natural units, since $\sigma_q = \sqrt{1/2}$,
\begin{equation}
	|\gamma(\mu_q,\mu_p)\> = \frac{1}{\pi^{\frac{1}{4}}} e^{-\frac{1}{2} \left(q-\mu_q\right)^2} e^{\imath \mu_p q} | q \>
\end{equation}
We note $\Gamma(\mu_q,\mu_p) = |\gamma(\mu_q,\mu_p)\> \<\gamma(\mu_q,\mu_p)| \in M(H)$ the corresponding density operator.

Note that, given that the Guassian wave packet saturates the uncertainty, we have $\sigma_p = \hbar / 2 \sigma_q$ (therefore, $\sigma_p = \sqrt{\hbar/2}$). In natural units, $\sigma_p = \sigma_q = \sqrt{1/2}$.

Let $G(H) \subset M(H)$ be the space of all mixed state that can be expressed as a mixture of Gaussian wave packets. That is, $\rho = \int f_\rho(q, p) \Gamma(q, p) dq dp$ where $f(q, p)$ is a distribution over phase space. We call $G(H)$ the space of Gaussian approximations. (Alternatively, we can define them with a probability measure.)

\section{GOAL}

Show that, as entropy increases, the space of Gaussian approximations becomes ``a good approximation'' to the space of mixed states. Also show that, as entropy increases, Hamiltonian evolution is ``a good approximation'' of unitary evolution. Part of the problem means defining what ``good approximation'' means.

\section{Possible approach}

Step 1. Define a distance function on $M(H)$. A possible choice is to use the Hilbert-Schmidt norm
\begin{equation}
	||\rho|| = \sqrt{\Tr \rho^2}
\end{equation}
which induces the distance
\begin{equation}
	D(\rho_1, \rho_2) = \sqrt{\Tr (\rho_1 - \rho_2)^2}
\end{equation}

Step 2. Define the distance of the best approximation for each mixed state. That is:
\begin{equation}
	BD(\rho) = \inf D(\rho, G(H))
\end{equation}
Then define the worst approximation at a given entropy, that is
\begin{equation}
	WD(I) = \sup BD(M(I)).
\end{equation}

We now have a function of entropy that describes how distant are the mixed state at that entropy to a corresponding Gaussian approximation. The first goal would be to show that it is a decreasing function and it tends to zero when entropy goes to infinity.

The other goal is to show that Hamiltonian evolution over the approximation is a good approximation of unitary evolution. We say that $\hat{\rho} \in G(H)$ is a good approximation of $\rho \in M(H)$ if $D(\rho, \hat{\rho}) \leq 2 WD(I(\rho))$ (something along these lines). Let $H = H(Q,P)$ be a Hamiltonian expressible in terms of position and momentum operators. Let $h(q,p)$ be the corresponding classical Hamiltonian as a function of position and momentum variables. The time evolution operator is $U(t)= e^{\imath \frac{Ht}{\hbar}}$. Let $\rho \in M(H)$ be a mixed state. Then, after a time $t$, the state will evolve into $U^\dagger(t) \rho U(t)$. We would like to show that if $\hat{\rho}$ is a good approximation of $\rho$ and $f_{\hat{\rho}}(q,p)$ is the integrable function associated with $\hat{\rho}$, then the Gaussian approximation given by the Hamiltonian evolution $\hat{\rho}(t) = \int f_{\hat{\rho}}(q,p,t) \Gamma(q,p) dq dp$ is a good approximation for $U^\dagger(t) \rho U(t)$.

\section{Possible problems}

The choice Hamiltonian may affect what entropy is needed. Consider classical Hamiltonian evolution. Given that a Gaussian remains a Gaussian only under linear transformation, a Gaussian will remain a Gaussian if and only if the Hamiltonian is quadratic. On the other hand, if the Lagrangian is chaotic, it will spread the initial Guassian into a wide-spread and complicated distribution. Given that entropy is strictly concave and Hamiltonian evolution conserves entropy, though we may be able to decompose the final distribution into the sum of infinitely many Gaussians, these must be Gaussians at lower entropy. Something similar may happen in the quantum case.

\section{Gaussian of Gaussians}

An interesting case to study could be a Gaussian distribution of Gaussian states. That is, let $\lambda$ be a dimensionless parameter. Let $\sigma_q = \sqrt{\lambda\hbar /2}$ expressed in meters and $\sigma_p = \lambda \hbar / 2 \sigma_q = \sqrt{\lambda\hbar /2}$, so that $\sigma_q\sigma_p = \lambda \hbar / 2$. We define the following family of Gaussians
\begin{equation}
	n_\lambda(q,p) = \frac{1}{\pi \lambda \hbar} e^{-\frac{\sigma_p^2 q^2 + \sigma_q^2 p^2}{\lambda^2 \hbar^2}}
\end{equation}
or, in natural units,
\begin{equation}
	n_\lambda(q,p) = \frac{1}{\pi \lambda} e^{-\frac{q^2 + p^2}{2 \lambda}}.
\end{equation}

The entropy is
\begin{equation}
	I(n_\lambda) = \ln 2 \pi e \lambda \hbar
\end{equation}
or
\begin{equation}
	I(n_\lambda) = \ln 2 \pi e \lambda
\end{equation}
in natural units.
Note that $I \to - \infty$ as $\lambda \to 0$ while $I \to + \infty$ as $\lambda \to + \infty$.

We can construct the mixed state
\begin{equation}
	N_\lambda = \int n_\lambda(q,p) \Gamma(q,p) dq dp.
\end{equation}
In principle, $I(N_\lambda)$ would describe the entropy of the mixed state as a function of $\lambda$. What we would like to show is that $I \to 0$ as $\lambda \to 0$. We expect the first derivative to be $0$ at that point and the second derivative to be positive. We hope to show that $I(N_\lambda)$ converges to $I(n_\lambda)$ as $\lambda \to +\infty$. Given that $I(N_\lambda)$ should be concave for small $\lambda$ and convex for large $\lambda$, we expect an inflection point for $\lambda$ a 2 or 3 times $\frac{1}{2 \pi e \hbar}$, which is where $I(n_\lambda)$ is equal to zero.

\section{Entropy increasing functions}

We saw that families of distributions that can be written as $f\left(\frac{q}{\sqrt{\lambda}}, \frac{p}{\sqrt{\lambda}}\right)$ play an important role. We want to find a motivation in terms of entropy.


Let $D = PD(X)$ be the set of all integrable distributions over phase space (one d.o.f.). Let $G$ be the group of canonical transformations. Therefore $g \in G$ is a function $g : D -> D$. We say a functional $b: D -> D$ is an “entropic magnifier” (name is a placeholder) if
\begin{itemize}
	\item it is linear - $b(a_1 f_1 + a_2 f_2) = a_1 b(f_1) + a_2 b(f_2) $
	\item preserves entropy ordering (i.e. $I(B(f_1)) \leq I(B(f_2))$ if and only if $I(f_1) \leq I(f_2)$) and entropy is non-decreasing (i.e. $I(B(f)) >= I(f)$).
	\item for every $g \in G$ there exists a $g’ \in G$ such that $b(g(f)) = g’(b(f))$
\end{itemize}

We can show that $b(f(q,p)) = f\left(\frac{q}{\sqrt{\lambda}}, \frac{p}{\sqrt{\lambda}}\right)$ is an entropy magnifier. In fact
\begin{align}
	I\left(f\left(\frac{q}{\sqrt{\lambda}}, \frac{p}{\sqrt{\lambda}}\right)\right) = I(f(q,p)) + \log(\lambda)
\end{align}
which means it is entropy increasing and preserves entropy ordering. A canonical transformation applied to the argument becomes a stretches canonical transformation applied to the result.

Can we claim that $b$ is a change of variable? That is,
\begin{align}
	b(f(q,p)) = b(\int f(u,v) \delta(u-q, v-p) du dv ) = \int f(u,v) b(\delta(u-q, v-p)) du dv
\end{align}
Therefore we need only to understand how the deltas are mapped. Given that deltas have the same entropy, minus infinity, they must be mapped to functions of the same entropy. Note that the linear combination of two delta functions also have minus infinite entropy. Since entropy is strictly concave, the linear combination of two functions with finite and equal entropy will always have greater entropy. Therefore delta diracs must be mapped to functions with zero support (minus infinite entropy).
\end{document}
